{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "709c5453",
      "metadata": {
        "id": "709c5453"
      },
      "source": [
        "## <font color='blue'>Text generation using an n-gram model</font>\n",
        "\n",
        "In this problem, we'll train a language model on Shakespeare and use it to generate text. Our model will be bare-bones but has the same underlying ideas as much more advanced models that are in wide use. Simply put, language models assign a likelihood (probability) to a given sentence. For example, the sentence \"dog is barking\" has a higher likelihood than \"bark dog is\", which is grammatically incoherent.\n",
        "\n",
        "### <font color='blue'>Bigram and trigram models</font>\n",
        "\n",
        "Suppose we have a sentence composed of words (or tokens) $x_1, x_2, ..., x_n$. We want to be able to compute the probability of this sequence, $P(x_1, x_2, ... x_n)$. The distribution can always be factored as\n",
        "\n",
        "$$  P(x_1, x_2, ... x_n)  =  P(x_1) \\prod_{i=2}^{n}P(x_i|x_{i-1}, x_{i-2}, ..., x_1) .$$\n",
        "\n",
        "However, these conditional probabilities are hard to model for even medium-sized vocabularies. One simplification is to make a <em>first-order Markov assumption</em>, which says that the probability of a word given all previous words is equal to the probability of the word given just the one word before it, that is,\n",
        "\n",
        "$$ P(x_i | x_1, x_2, ..., x_{i-1}) = P(x_i | x_{i-1}). $$\n",
        "\n",
        "This allows us to factor\n",
        "\n",
        "$$ P(x_1, x_2, ... x_n)  = \\prod_{i=1}^{n}P(x_i|x_{i-1}) $$\n",
        "\n",
        "where we take $x_0$ to be a special \"START\" symbol.\n",
        "\n",
        "The above formulation is called a <em>bigram</em> language model, since it is based on pairs of consecutive words. If we expand the context to include the two previous words, we get a <em>trigram</em> model,\n",
        "    \n",
        "$$ P(x_1, x_2, ... x_n)  = \\prod_{i=1}^{n}P(x_i|x_{i-2}, x_{i-1}) $$\n",
        "\n",
        "where $x_{-1}, x_0$ are special start symbols.\n",
        "\n",
        "In the same way, we can define an <em>n-gram</em> model by expanding the context to include the $n-1$ previous words.\n",
        "\n",
        "### <font color='blue'>Learning an $n$-gram model</font>\n",
        "\n",
        "In this problem, we will use <em>maximum-likelihood estimation</em> to learn an $n$-gram model. For this, we simply need to count the number of occurrences of each $n$-gram and $n-1$ gram in the corpus.\n",
        "\n",
        "For instance, to train a trigram model, let $c(u,v,w)$ be the number of times that trigram $(u,v,w)$ is seen in the training corpus and let $c(u,v)$ be the number of times that bigram $(u,v)$ is seen. The maximum likelihood estimate of $P(w|u,v)$ is then\n",
        "$$ P(w|u,v) = \\frac{c(u,v,w)}{c(u,v)} .$$\n",
        "Of course, we might want to smooth this using Laplace smoothing or something similar."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82556bde",
      "metadata": {
        "id": "82556bde"
      },
      "source": [
        "### <font color='blue'>Reading in the data</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47320ab7",
      "metadata": {
        "id": "47320ab7"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ReY6HBzrIG5",
        "outputId": "d7641106-b96f-4c41-fc01-9cb8121a6085"
      },
      "id": "9ReY6HBzrIG5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33029d64",
      "metadata": {
        "id": "33029d64"
      },
      "source": [
        "The provided text file `shakespeare.txt` contains a selection of Shakepeare's sonnets and plays. It also contains stage directions, copyright notices, and various other things that we won't bother removing. You should take a quick look at this file. We will read in a list of all sentences in the file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15b3fff5",
      "metadata": {
        "id": "15b3fff5"
      },
      "outputs": [],
      "source": [
        "sentences = []\n",
        "with open(\"/content/drive/My Drive/DSC 257R/text-gen/shakespeare.txt\", \"r\", encoding=\"utf8\") as f:\n",
        "    text = f.read()\n",
        "    text = text.split(\".\")\n",
        "    for sentence in text:\n",
        "        sentence += \".\"\n",
        "        sentences.append(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4761bba8",
      "metadata": {
        "id": "4761bba8"
      },
      "source": [
        "Here is an example of one of the sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34dc42cc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "34dc42cc",
        "outputId": "c1498a23-7bac-428a-f24b-50f8d7e216f0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\n\\n                     82\\n  I grant thou wert not married to my muse,\\n  And therefore mayst without attaint o'erlook\\n  The dedicated words which writers use\\n  Of their fair subject, blessing every book.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "sentences[200]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d429398c",
      "metadata": {
        "id": "d429398c"
      },
      "source": [
        "Let's set aside the very first sentence (we'll use it later). We'll extract all words from the remainder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07886413",
      "metadata": {
        "id": "07886413"
      },
      "outputs": [],
      "source": [
        "test_play = sentences[0]\n",
        "sentences = sentences[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e81bae7d",
      "metadata": {
        "id": "e81bae7d"
      },
      "outputs": [],
      "source": [
        "words = set()\n",
        "for sent in sentences:\n",
        "    for word in sent.split():\n",
        "        words.add(word)\n",
        "vocab = list(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dfb53f3",
      "metadata": {
        "id": "4dfb53f3"
      },
      "source": [
        "### <font color='blue'>Class: NGramModel</font>\n",
        "\n",
        "Next, we define a class that will hold an n-gram model. Go through the methods of this class and make sure you understand what is happening in each method as you'll be asked to use these later. Also take note of the data structures used in various methods.\n",
        "\n",
        "Some notes:\n",
        "* The `generate_n_grams` method takes a sequence (list) of tokens and return all of its ngrams. For example, for `tokens = ['Unsupervised',  'Learning', 'is', 'fun', '.']`, the trigrams would be `[(['START', 'START'], 'Unsupervised'), (['START', 'Unsupervised'], 'Learning') ...]`.\n",
        "* The `get_prob` method takes in a list of context tokens and a target token and returns the probability of the target given the context.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fb239f2",
      "metadata": {
        "id": "2fb239f2"
      },
      "outputs": [],
      "source": [
        "from typing import List, Dict, Tuple\n",
        "\n",
        "class NGramModel(object):\n",
        "    def __init__(self, n: int) -> None:\n",
        "        self.n = n\n",
        "        self.n_grams = dict() # Stores unique n-grams\n",
        "        self.context_count = dict() # Stores count of contexts\n",
        "        self.ngram_count = dict() # Stores count of ngrams: (context, token)\n",
        "\n",
        "    def tokenize(self, text: str) -> List[str]:\n",
        "        # Tokenize the given sentence 'text'\n",
        "        # Treat punctuation as a separate token.\n",
        "        # Add space before punctuation.\n",
        "        # Split using spaces.\n",
        "\n",
        "        for ch in string.punctuation:\n",
        "            text = text.replace(ch, \" \" + ch)\n",
        "        tokens = text.strip().split()\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def generate_n_grams(self, tokens: List[str]) -> List[Tuple[List[str], str]]:\n",
        "        # Generate all n-grams from the given list of tokens\n",
        "        # Insert (n-1) <START> tokens before each sentence\n",
        "        tokens = (self.n-1)*[\"<START>\"] + tokens\n",
        "        n_grams = list()\n",
        "\n",
        "        \"\"\"\n",
        "        n_grams is a list where each element is\n",
        "        ([n-1 context tokens], token)\n",
        "        \"\"\"\n",
        "        for i in range(len(tokens)-self.n+1):\n",
        "            n_grams.append((tokens[i: i+self.n-1], tokens[i+self.n-1]))\n",
        "\n",
        "        return n_grams\n",
        "\n",
        "    def fit(self, text: str) -> None:\n",
        "        # Takes a sentence 'text'\n",
        "        # Generates all n-grams in the sentence\n",
        "        # Then updates counts\n",
        "        new_n_grams = self.generate_n_grams(self.tokenize(text))\n",
        "        for context, target in new_n_grams:\n",
        "            # Add context to context dict and store count\n",
        "            if tuple(context) in self.context_count:\n",
        "                self.context_count[tuple(context)] += 1.0\n",
        "            else:\n",
        "                self.context_count[tuple(context)] = 1.0\n",
        "\n",
        "            # Save unique n_grams.\n",
        "            # This part is used for generation only.\n",
        "            if tuple(context) in self.n_grams:\n",
        "                if target not in self.n_grams[tuple(context)]:\n",
        "                    self.n_grams[tuple(context)].append(target)\n",
        "            else:\n",
        "                self.n_grams[tuple(context)] = [target]\n",
        "\n",
        "            # Store n_gram counts\n",
        "            new_n_gram = (tuple(context), target)\n",
        "            if new_n_gram in self.ngram_count:\n",
        "                self.ngram_count[new_n_gram] += 1.0\n",
        "            else:\n",
        "                self.ngram_count[new_n_gram] = 1.0\n",
        "\n",
        "\n",
        "\n",
        "    def get_prob(self, context: List[str], target: str) -> float:\n",
        "        \"\"\"\n",
        "        Calculates the probability of the target token\n",
        "        given the context.\n",
        "        \"\"\"\n",
        "        prob = self.ngram_count[(tuple(context), target)] / self.context_count[tuple(context)]\n",
        "        return prob\n",
        "\n",
        "\n",
        "\n",
        "    def predict_token(self, context: List[str]) -> str:\n",
        "        \"\"\"\n",
        "        Predict the next token given context.\n",
        "        A slight randomness ensures we generate a diverse token\n",
        "        with the same context.\n",
        "        \"\"\"\n",
        "        r = random.random()\n",
        "        # store the probability of each token.\n",
        "        token_probs = dict()\n",
        "\n",
        "        try:\n",
        "            tokens_of_interest = self.n_grams[tuple(context)]\n",
        "            for token in tokens_of_interest:\n",
        "                token_probs[token] = self.get_prob(context, token)\n",
        "        except KeyError: # similar to Laplace smoothing; returns a random word from the vocab.\n",
        "            ridx = random.randint(0, len(words))\n",
        "            return vocab[ridx]\n",
        "\n",
        "\n",
        "        sum = 0.0\n",
        "        for key in sorted(token_probs):\n",
        "            sum += token_probs[key]\n",
        "            # When the probability sum is > random number\n",
        "            # return the current token.\n",
        "            if sum > r:\n",
        "                return key\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a226b5e9",
      "metadata": {
        "id": "a226b5e9"
      },
      "source": [
        "### <font color='blue'>Routines for fitting data and generating text</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f30c2868",
      "metadata": {
        "id": "f30c2868"
      },
      "source": [
        "`create_and_fit_model` defines an n-gram model and fits it to data. Its parameters are `n` (the order of the model) and `sentences` (collection of sentences).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a183de4",
      "metadata": {
        "id": "3a183de4"
      },
      "outputs": [],
      "source": [
        "def create_and_fit_model(n, sentences):\n",
        "    \"\"\"\n",
        "    This is the key function that defines and fits an n-gram model.\n",
        "    It takes in n and a list of sentences.\n",
        "    It creates an n-gram model and then calls the `fit` method on\n",
        "    one sentence at a time to generate counts.\n",
        "    \"\"\"\n",
        "    model = NGramModel(n)\n",
        "    for sent in sentences:\n",
        "        model.fit(sent)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d8a5bb0",
      "metadata": {
        "id": "7d8a5bb0"
      },
      "source": [
        "`generate_text` uses an n-gram model to generate text, starting from a prompt (which might be empty). It takes as input the `model`, the number of words to generate (`n_outs`) and the `prompt`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b51c8b9",
      "metadata": {
        "id": "0b51c8b9"
      },
      "outputs": [],
      "source": [
        "def generate_text(model: NGramModel, n_outs: int, prompt=None) -> str:\n",
        "    \"\"\"\n",
        "    Generates n_outs words using the trained\n",
        "    ngram model.\n",
        "    \"\"\"\n",
        "    n = model.n\n",
        "    # All sentence are initialized with the <START> token\n",
        "\n",
        "    if prompt is not None:\n",
        "        prompt_tokens = model.tokenize(prompt)\n",
        "        context_queue = prompt_tokens[-(n-1):]\n",
        "    else:\n",
        "        context_queue = (n-1) * [\"<START>\"]\n",
        "    result = list()\n",
        "\n",
        "    for _ in range(n_outs):\n",
        "        pred_token = model.predict_token(context_queue)\n",
        "        result.append(pred_token)\n",
        "\n",
        "        context_queue.pop(0)\n",
        "\n",
        "        if pred_token == \".\":\n",
        "            # If sentence done. Start a new sentence.\n",
        "            context_queue = (n-1) * [\"<START>\"]\n",
        "        else:\n",
        "            context_queue.append(pred_token)\n",
        "\n",
        "    return \" \".join(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc401f79",
      "metadata": {
        "id": "bc401f79"
      },
      "outputs": [],
      "source": [
        "def print_generated_text(model):\n",
        "    \"\"\"\n",
        "    Prints a 100-word blurb from the provided model.\n",
        "    \"\"\"\n",
        "    num_gen_words = 100\n",
        "    print(f'{\"=\"*50}\\nGenerated text:')\n",
        "    print(\"\\n\")\n",
        "    print(generate_text(model, num_gen_words))\n",
        "    print(f'{\"=\"*50}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "beb221db",
      "metadata": {
        "id": "beb221db"
      },
      "source": [
        "### <font color='blue'>Experimenting with text generation</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fbece21",
      "metadata": {
        "id": "5fbece21"
      },
      "outputs": [],
      "source": [
        "## SOLUTION:\n",
        "\n",
        "# Create and fit the bigram (n=2) model\n",
        "bigram_model = create_and_fit_model(n=2, sentences=sentences)\n",
        "\n",
        "# Create and fit the trigram (n=3) model\n",
        "trigram_model = create_and_fit_model(n=3, sentences=sentences)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text using the bigram model\n",
        "print_generated_text(bigram_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPqclg9kvOBL",
        "outputId": "f4c3d24b-58cc-4113-9fda-49637bf48e48"
      },
      "id": "BPqclg9kvOBL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Generated text:\n",
            "\n",
            "\n",
            "TOUCHSTONE . Exeunt ACT IV . CONSTANCE . Cousin Hamlet is proof ! and to blame in this . And ceremoniously let them all my fortune . [She curtsies there 's day ! To lengthen 'd , And level ' heart can with a lugg 'd you , gentle one soft , I am at least , wrench 'd , In filial tenderness will or four set The swifter toward the park with straining on this town ? ROSALIND . Madam ! ' fortune ! Exeunt ATTENDANTS PAULINA . COURTEZAN . Is your spirit go learn 'd upon yourself come\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text using the trigram model\n",
        "print_generated_text(trigram_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6f7LUCMvQft",
        "outputId": "c0ebba1a-7c49-4796-9510-23d02fd40142"
      },
      "id": "o6f7LUCMvQft",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Generated text:\n",
            "\n",
            "\n",
            "LEONTES . APEMANTUS . [To ABERGAVENNY ] The trumpets have sounded the very latest counsel That ever said How that the lunacy is so very a fool . PORTIA . Like an old hat , a very virtuous maid , and my auguring hope Says it is my true date . Why stay we now possess 'd Of that same Faulconbridge . We will , it must be present at this hour joy o 'er -worn widow , nor no man ever saw him so ? SIR TOBY . CRESSIDA . Such a mad tale he told to -day with\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc6cf7ec",
      "metadata": {
        "id": "dc6cf7ec"
      },
      "source": [
        "### <font color='blue'>Computing likelihoods</font>\n",
        "\n",
        "In the part, we'll calculate the log-likelihood of a sentence using the models that we just created. This will give us a concrete indication of why incorporating more context is helpful.\n",
        "For a given sentence $s$ with $n$ tokens $(x_1, \\ldots, x_n)$, the log-likelihood is defined as\n",
        "$$ L(s) = \\sum_{i=1}^{n}\\log p(x_i|\\mbox{context}) $$\n",
        "For a unigram model, this maximum-likelihood estimates of $p(x_i)$ would simply be\n",
        "$$ p(x_i) = \\frac{c(x_i)}{N}$$\n",
        "where N is the total number of words in the dataset and $c(x_i)$ is the number of occurrences of $x_i$.  \n",
        "Similarly, for a bigram model, we have\n",
        "$$ p(x_i|\\mbox{context}) = \\frac{c(x_{i-1}, x_i)}{c(x_{i-1})} .$$\n",
        "In our code, these probabilities are provided by `get_prob` in `NGramModel`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1e5d9e1",
      "metadata": {
        "id": "a1e5d9e1"
      },
      "outputs": [],
      "source": [
        "def model_ll(model, text):\n",
        "    \"\"\"\n",
        "    Takes in an n-gram model and a sample text.\n",
        "    Returns the log-likelihood of the text under the given model.\n",
        "    \"\"\"\n",
        "    import math\n",
        "    ll = 0\n",
        "    # raise NotImplementedError\n",
        "    ## SOLUTION:\n",
        "    # 1. Tokenize the text.\n",
        "    tokens = model.tokenize(text)\n",
        "    # 2. Generate ngrams for the tokens.\n",
        "    ngrams = model.generate_n_grams(tokens)\n",
        "    # 3. Loop through the ngrams, calculate log prob for each ngram and update ll.\n",
        "    for context, target in ngrams:\n",
        "        # Calculate probability of the target word given the context\n",
        "        prob = model.get_prob(context, target)\n",
        "\n",
        "        # Avoid log(0) by adding a small epsilon if prob is zero\n",
        "        if prob > 0:\n",
        "            ll += math.log(prob)\n",
        "        else:\n",
        "            ll += math.log(1e-10)  # or another small value to approximate zero probability\n",
        "\n",
        "    return ll"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e0fc28b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e0fc28b",
        "outputId": "a199bcf8-32a1-486a-d248-4b7f6ab2dcfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log-likelihood of 'And on a love-book pray for my success?' under a unigram model: -65.86143429059021\n",
            "Log-likelihood of 'And on a love-book pray for my success?' under a bigram model: -45.879305709530776\n"
          ]
        }
      ],
      "source": [
        "text = \"And on a love-book pray for my success?\"\n",
        "## SOLUTION:\n",
        "# Calculate log-likelihood of the text under a unigram model\n",
        "unigram_model = create_and_fit_model(n=1, sentences=sentences)\n",
        "unigram_ll = model_ll(unigram_model, text)\n",
        "print(f\"Log-likelihood of '{text}' under a unigram model: {unigram_ll}\")\n",
        "# Calculate log-likelihood of the text under a bigram model\n",
        "bigram_model = create_and_fit_model(n=2, sentences=sentences)\n",
        "bigram_ll = model_ll(bigram_model, text)\n",
        "print(f\"Log-likelihood of '{text}' under a bigram model: {bigram_ll}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c1804fd",
      "metadata": {
        "id": "1c1804fd"
      },
      "source": [
        "### <font color='blue'>Text completion</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69f9e6e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69f9e6e2",
        "outputId": "85b76542-8b40-43c5-f8cf-eac6c8f8794f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THE SONNETS\n",
            "\n",
            "by William Shakespeare\n",
            "\n",
            "\n",
            "\n",
            "                     1\n",
            "  From fairest creatures we desire increase,\n",
            "  That thereby beauty's rose might never die,\n",
            "  But as the riper should by time decease,\n",
            "  His tender heir might bear his memory:\n",
            "  But thou contracted to thine own bright eyes,\n",
            "  Feed'st thy light's flame with self-substantial fuel,\n",
            "  Making a famine where abundance lies,\n",
            "  Thy self thy foe, to thy sweet self too cruel:\n",
            "  Thou that art now the world's fresh ornament,\n",
            "  And only herald to the gaudy spring,\n",
            "  Within thine own bud buriest thy content,\n",
            "  And tender churl mak'st waste in niggarding:\n",
            "    Pity the world, or else this glutton be,\n",
            "    To eat the world's due, by the grave and thee.\n"
          ]
        }
      ],
      "source": [
        "# QUESTION\n",
        "print(test_play)\n",
        "prompt = \"From fairest creatures we desire increase\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## SOLUTION:\n",
        "# Generate text from the bigram model\n",
        "bigram_generated_text = generate_text(bigram_model, n_outs=20, prompt=prompt)\n",
        "print(bigram_generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3HWcEbS7WCq",
        "outputId": "f0e30318-59fd-41c6-8a6c-e51fd2513340"
      },
      "id": "S3HWcEbS7WCq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "with her straight . While sense possesses ? What sum of the stones , it . Are ye Power into\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text from the tigram model\n",
        "trigram_generated_text = generate_text(trigram_model, n_outs=20, prompt=prompt)\n",
        "print(trigram_generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zyure0Vv7erH",
        "outputId": "89f49068-b61e-4b3c-a237-a0c0e7d1ccd7"
      },
      "id": "Zyure0Vv7erH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "marriage! Hell Sir? madded. drop, MIRANDA, easiness Grey? air? encounter. lull'd Princess- aqua-vitae lion Numa's BANDITTI. hanged fickleness. Fitzwater, unwittingly,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09317587",
      "metadata": {
        "id": "09317587"
      },
      "source": [
        "There are various tweaks that would improve this model: for instance, careful <em>smoothing</em> and using longer-range dependencies (via variable-length Markov models such as <em>probabilistic suffix trees</em>). The next big boost in performance would come from replacing tabular estimates of conditional probabilities $P(x_i|x_1, ...., x_{i-1})$ by <em>recurrent neural nets</em>."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "toc": {
      "colors": {
        "hover_highlight": "#DAA520",
        "navigate_num": "#000000",
        "navigate_text": "#333333",
        "running_highlight": "#FF0000",
        "selected_highlight": "#FFD700",
        "sidebar_border": "#EEEEEE",
        "wrapper_background": "#FFFFFF"
      },
      "moveMenuLeft": true,
      "nav_menu": {
        "height": "189px",
        "width": "252px"
      },
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 4,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false,
      "widenNotebook": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}